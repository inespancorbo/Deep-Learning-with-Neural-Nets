{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT-notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwSWA7XDD40s",
        "colab_type": "text"
      },
      "source": [
        "Sub Task A results: https://arxiv.org/pdf/1903.08983.pdf\n",
        "\n",
        "Data from: https://github.com/sandro272/SemEval2019-OffensEval/tree/51dde8c38b512d5fb536fd74b2afd3dc7ed73831/train_data\n",
        "\n",
        "https://github.com/sandro272/SemEval2019-OffensEval/tree/51dde8c38b512d5fb536fd74b2afd3dc7ed73831/test_data\n",
        "\n",
        "pre-processing: https://github.com/sandro272/SemEval2019-OffensEval/blob/51dde8c38b512d5fb536fd74b2afd3dc7ed73831/code/demo.py#L9\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "senFKVtyUALS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMppP12JUSVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch   \n",
        "from torchtext import data, datasets \n",
        "from torchtext.data import TabularDataset \n",
        "import pandas as pd\n",
        "from torchtext.vocab import Vectors\n",
        "from torch.nn import init\n",
        "import torch.nn as nn\n",
        "from torchtext.vocab import Vectors\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import get_linear_schedule_with_warmup,get_cosine_schedule_with_warmup,get_constant_schedule_with_warmup,get_cosine_with_hard_restarts_schedule_with_warmup\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig,AdamWeightDecay\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = \"cuda\"\n",
        "else:\n",
        "  device = \"cpu\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfdnNp0EIB07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVP72Eb1IGxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls \"/content/drive/My Drive/576-project\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeyLr9y_IVpf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(2020)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riS74LzPDOpN",
        "colab_type": "text"
      },
      "source": [
        "optimizer &learning rate scheduler\n",
        "\n",
        "**hyperparameters**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DdfO4ffLnDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 5\n",
        "learning_rate = 2e-5\n",
        "eps = 1e-8\n",
        "batch_size = 64\n",
        "bert_type = 'bert-base-uncased'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCMOEw19KmiA",
        "colab_type": "text"
      },
      "source": [
        "read traindata and testdata"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fuh5ZCuLIW2x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "traindata = pd.read_csv('/content/drive/My Drive/576-project/traindata.csv')\n",
        "testdata = pd.read_csv('/content/drive/My Drive/576-project/testdata.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ls9FXb_sIwXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "traindata.head()\n",
        "testdata.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNAT7UkPIlgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "train_sentences = traindata.tweet.values\n",
        "train_labels = traindata.label.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECqZ-mgmJU5-",
        "colab_type": "text"
      },
      "source": [
        "tokenization & input formatting\n",
        "1. bert tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzRj2wB9JTcv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(bert_type, do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrpqVW-sM4o3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in train_sentences:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrhnqWP_QqgJ",
        "colab_type": "text"
      },
      "source": [
        "since the longest sentences here is 115, set the maximum length to 128. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMEX7DBWQps9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in train_sentences:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 128,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmTF8ioxK8M6",
        "colab_type": "text"
      },
      "source": [
        "create train and validation set (the proportion is 8:2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFA3UfqCTiSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFZj465aUzsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataloader = DataLoader(train_dataset,sampler = RandomSampler(train_dataset), batch_size = batch_size)\n",
        "validation_dataloader = DataLoader(val_dataset,sampler = SequentialSampler(val_dataset),batch_size = batch_size )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF-5V3VDVvMv",
        "colab_type": "text"
      },
      "source": [
        " Use BertForSequenceClassification\n",
        "\n",
        " refer: https://huggingface.co/transformers/v2.2.0/model_doc/bert.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riig5VBUVs6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    bert_type, \n",
        "    num_labels = 2,   \n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False, \n",
        ")\n",
        "\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjOVvOhMYkpT",
        "colab_type": "text"
      },
      "source": [
        "train the classification model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XArAzUfJxQQi",
        "colab_type": "text"
      },
      "source": [
        "optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPO_WLddDEGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(),lr = learning_rate,eps=eps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ds35z8vAV3yr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler =  get_cosine_schedule_with_warmup(optimizer,num_warmup_steps = 1,num_training_steps = total_steps,num_cycles=0.6 )\n",
        "#scheduler =  get_constant_schedule_with_warmup(optimizer,num_warmup_steps = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUhOo5cVMMN7",
        "colab_type": "text"
      },
      "source": [
        "training loop\n",
        "\n",
        " This training code is based on the `run_glue.py` script here:\n",
        "https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhGBW-CsMLMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_val = 576\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4yxZ65Mk7fk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_stats = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjUAxMBFJmd8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpEC6pkg7vs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f1score(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    TP = np.sum((labels_flat == 1) & (pred_flat == 1))\n",
        "    FP = np.sum((labels_flat == 0) & (pred_flat == 1))\n",
        "    FN = np.sum((labels_flat == 1) & (pred_flat == 0))  \n",
        "    precision = TP / (TP + FP)\n",
        "    #precision_0\n",
        "    recall = TP / (TP + FN)\n",
        "    #recall_0 =\n",
        "    f1 = 2 * precision * recall / (precision + recall)\n",
        "    return np.sum(f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCKu3nIYpvXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f1_score_macro(preds, labels):\n",
        "\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    f1 = f1_score(labels_flat, pred_flat,average='macro')\n",
        "    return np.sum(f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXRR7-5MMuAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "    total_train_accuracy = 0\n",
        "    total_train_f1 = 0\n",
        "    total_train_f1_macro = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % 20 == 0 and not step == 0:\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.   '.format(step, len(train_dataloader)))\n",
        "            print(\"  batch loss: {0:.4f}\".format(total_train_loss/step))\n",
        "            if(total_train_loss/step < 0.41):\n",
        "               break\n",
        "            \n",
        "\n",
        "        \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "        (loss, logits) = model(b_input_ids,token_type_ids=None,attention_mask=b_input_mask,labels=b_labels)\n",
        "        \n",
        "        total_train_loss += loss.item()\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy() \n",
        "       \n",
        "\n",
        "        total_train_accuracy += flat_accuracy(logits, label_ids)\n",
        "        total_train_f1 += f1score(logits, label_ids)\n",
        "        total_train_f1_macro += f1_score_macro(logits, label_ids)\n",
        "        \n",
        "        \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        " \n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    avg_train_accuracy = total_train_accuracy / len(train_dataloader)            \n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.4f}\".format(avg_train_loss))\n",
        "    \n",
        "    avg_train_f1 = total_train_f1 / len(train_dataloader)\n",
        "    print(\"  Average training F1: {0:.4f}\".format(avg_train_f1))   \n",
        "\n",
        "    avg_train_f1_macro = total_train_f1_macro / len(train_dataloader)\n",
        "    print(\"  Average training macro-F1: {0:.4f}\".format(avg_train_f1_macro))   \n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "    model.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    total_eval_f1 = 0\n",
        "    total_eval_f1_macro = 0\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        with torch.no_grad():        \n",
        "            (loss, logits) = model(b_input_ids,token_type_ids=None,attention_mask=b_input_mask,labels=b_labels)     \n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "        total_eval_f1 += f1score(logits, label_ids)\n",
        "\n",
        "        total_eval_f1_macro += f1_score_macro(logits, label_ids)\n",
        "        \n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.4f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader) \n",
        "    print(\"  Validation Loss: {0:.4f}\".format(avg_val_loss))\n",
        "\n",
        "    avg_val_f1 = total_eval_f1 / len(validation_dataloader) \n",
        "    print(\"  Validation F1: {0:.4f}\".format(avg_val_f1))\n",
        "\n",
        "    avg_val_f1_macro = total_eval_f1_macro / len(validation_dataloader) \n",
        "    print(\"  Validation macro-F1: {0:.4f}\".format(avg_val_f1_macro))\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Train. Accur.': avg_train_accuracy,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Train F1.': avg_train_f1,\n",
        "            'Valid F1.': avg_val_f1,\n",
        "            'Train macro-F1':avg_train_f1_macro,\n",
        "            'Valid macro-F1':avg_val_f1_macro,\n",
        "        }\n",
        "    )\n",
        "print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CRdP8W7Y_Dv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "df_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qs8GlzYRV8wy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(1, 3, 1)\n",
        "ax2 = fig.add_subplot(1, 3, 2)\n",
        "ax3 = fig.add_subplot(1, 3, 3)\n",
        "\n",
        "ax1.plot(df_stats['Valid. Loss'], label='validation loss')\n",
        "ax1.plot(df_stats['Training Loss'], label='training loss')\n",
        "\n",
        "ax2.plot(df_stats['Train. Accur.'], label='training accuracy')\n",
        "ax2.plot(df_stats['Valid. Accur.'], label='validation accuracy')\n",
        "\n",
        "ax3.plot(df_stats['Train macro-F1'], label='training macro-F1-score.')\n",
        "ax3.plot(df_stats['Valid macro-F1'], label='validation macro-F1-score.')\n",
        "\n",
        "\n",
        "ax1.set_xlabel('Epoch (s)')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Loss')\n",
        "ax1.set_xticks([1,2,3,4,5])\n",
        "ax1.legend()\n",
        "\n",
        "\n",
        "ax2.set_xlabel('Epoch (s)')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('Accuracy')\n",
        "ax2.set_xticks([1,2,3,4,5])\n",
        "ax2.legend()\n",
        "\n",
        "ax3.set_xlabel('Epoch (s)')\n",
        "ax3.set_ylabel('macro-F1')\n",
        "ax3.set_title('macro-F1')\n",
        "ax3.set_xticks([1,2,3,4,5])\n",
        "ax3.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_wqq4cVetn0",
        "colab_type": "text"
      },
      "source": [
        "test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME18XkATetJd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create sentence and label lists\n",
        "sentences = testdata.tweet.values\n",
        "labels = testdata.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D95JfhAhT-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for sent in sentences:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 128,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "        \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "  \n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kFwgN42Znm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix,plot_confusion_matrix\n",
        "\n",
        "def TP(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    TP = np.sum((labels_flat == 1) & (pred_flat == 1))\n",
        "    return(TP)\n",
        "def FP(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    FP = np.sum((labels_flat == 0) & (pred_flat == 1))\n",
        "    return(FP)\n",
        "def FN(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    FN = np.sum((labels_flat == 1) & (pred_flat == 0))\n",
        "    return(FN)\n",
        "def TN(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    TN = np.sum((labels_flat == 0) & (pred_flat == 0))\n",
        "    return(TN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unW9h5COae_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()\n",
        "predictions , true_labels = [], []\n",
        "# Predict \n",
        "total_test_accuracy = 0\n",
        "total_test_f1 = 0\n",
        "total_test_f1_macro = 0\n",
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "tn = 0\n",
        "\n",
        "for batch in prediction_dataloader:\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  with torch.no_grad():\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "  logits = outputs[0]\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  total_test_accuracy += flat_accuracy(logits, label_ids)\n",
        "  avg_test_accuracy = total_test_accuracy / len(prediction_dataloader)\n",
        "  \n",
        "  total_test_f1 += f1score(logits, label_ids)\n",
        "  avg_test_f1 = total_test_f1 / len(prediction_dataloader)\n",
        "  test_f1 = f1score(logits, label_ids)\n",
        "  #print(\" test f1: {0:.4f}\".format(test_f1))\n",
        "  \n",
        "\n",
        "  total_test_f1_macro += f1_score_macro(logits, label_ids)\n",
        "  avg_test_f1_macro = total_test_f1_macro / len(prediction_dataloader)\n",
        "  test_f1_macro = f1_score_macro(logits, label_ids)\n",
        "  #print(\" test f1_macro: {0:.4f}\".format(test_f1_macro))\n",
        "\n",
        "  tp += TP(logits, label_ids)\n",
        "  fp += FP(logits, label_ids)\n",
        "  fn += FN(logits, label_ids)\n",
        "  tn += TN(logits, label_ids) \n",
        "  \n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print(\" test Accuracy: {0:.4f}\".format(avg_test_accuracy))\n",
        "print(\" test f1: {0:.4f}\".format(avg_test_f1))\n",
        "print(\" test f1_macro: {0:.4f}\".format(avg_test_f1_macro))\n",
        "#print(\" test tn: {0:.4f}\".format(avg_tn))\n",
        "#print(\" test fp: {0:.4f}\".format(avg_fp))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h5Z_BOslFEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tp,fp,fn,tn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-vHl7o_apgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred1 = np.array([[163,46],[77,574]])\n",
        "pred1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPp7b909A0b2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,cmap=plt.cm.Blues):\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "    \n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "    return ax\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK9bTHCCBL0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(pred1,classes = [\"OFFENSIVE\",\"NOT\"])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}